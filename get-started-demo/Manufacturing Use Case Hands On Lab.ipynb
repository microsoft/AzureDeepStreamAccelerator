{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manufacturing Use Case using TAO MaskRCNN\n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data. In this example we will be training an instance segmentation model for tracking defects along a manufacturing line. \n",
    "\n",
    "<!-- <img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Lab Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Prepare a dataset which was labeled in Azure Machine Learning for use with the TAO Toolkit\n",
    "* Take a pretrained resnet50 model and train a MaskRCNN model on COCO dataset\n",
    "* Evaluate the trained model\n",
    "* Optimize the trained model by pruning and retraining\n",
    "* Export the trained model to a .etlt file for deployment to DeepStream\n",
    "* Integrate the exported model into a DeepStream Pipeline\n",
    "\n",
    "## Prerequisites\n",
    "> **NOTE:** In order to complete this hands on lab, we assume you have already completed the steps mentioned in the [README.md](README.md) document of this repository. Please complete the steps mentioned there prior to proceeding further. Additionally this notebook should be opened from the conda environment you configured for use with the tao toolkit.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "This notebook shows an example use case for instance segmentation using the Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "1. [Configure Key Directories](#head-1)\n",
    "2. [Instal Python Dependencies](#head-2)\n",
    "3. [Install Nvidia NGC CLI Tool](#head-3)\n",
    "4. [Download and Prepare Training Data](#head-4)\n",
    "5. [Download pre-trained model](#head-5)\n",
    "6. [Provide the TAO training specification](#head-6)\n",
    "7. [Train a MaskRCNN model](#head-7)\n",
    "8. [Evaluate trained model](#head-8)\n",
    "9. [Prune the model](#head-9)\n",
    "10. [Retrain pruned models](#head-10)\n",
    "11. [Evaluate retrained model](#head-11)\n",
    "12. [Export the model for use with DeepStream](#head-12)\n",
    "13. [Run a deepstream pipeline with the model](#head-13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Key Directories <a class=\"anchor\" id=\"head-1\"></a>\n",
    "---\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$PROJECT_DIR/maskrcnn`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The cell below configures the key project directories required for running this notebook. A table summarizing the directories and expected values are listed below.\n",
    "\n",
    "|Directory Name|Description|\n",
    "|--------------|-----------|\n",
    "|PROJECT_DIR|General project directory. This should point to the root of your workspace|\n",
    "|DATA_DIR|Sub-directory of your project where the images and annotations are stored|\n",
    "|AML_DATA_DIR|Sub-directory of DATA_DIR where the raw image data should be downloaded|\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$EXPERIMENT_DIR` or `$AML_DATA_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n",
    "\n",
    "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT_DIR'] = os.environ['PWD'] + \"/workspace\"\n",
    "os.environ['DATA_DIR'] = os.environ['PROJECT_DIR'] + \"/data\"\n",
    "os.environ['AML_DATA_DIR'] = os.environ['PROJECT_DIR'] + \"/data/raw-images\"\n",
    "os.environ['EXPERIMENT_DIR'] = os.environ['PROJECT_DIR'] + \"/models/maskrcnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure TAO mount directories for container bindings\n",
    "\n",
    "The TAO toolkit requires a file which defines the directories to be mounted to the container for the various tasks. This file is located in `~/.tao_mounts.json`. The cell below uses the previously specified directories to generate this file for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# map the workspace directory from this repo to the location in the container\n",
    "\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Python dependencies <a class=\"anchor\" id=\"head-2\"></a>\n",
    "---\n",
    "\n",
    "This lab requires several key packages in order to run the experience. The below cell will install the following packages and any dependencies into your current conda environment.\n",
    "\n",
    "### Installed Packages\n",
    "- nvidia-tao\n",
    "- azure-storage-blob\n",
    "- pycocotools\n",
    "- absl-py\n",
    "- tensorflow-object-detection-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages from requirements file\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the Tao Launcher is available\n",
    "\n",
    "Run the below command to validate that the tao launcher tool is available. The output of the command should appear similar to the below:\n",
    "\n",
    "\n",
    "#### Sample Output\n",
    "```text\n",
    "$ tao info\n",
    "\n",
    "Configuration of the TAO Toolkit Instance\n",
    "dockers: ['nvidia/tao/tao-toolkit-tf', 'nvidia/tao/tao-toolkit-pyt', 'nvidia/tao/tao-toolkit-lm']\n",
    "format_version: 2.0\n",
    "toolkit_version: 3.22.05\n",
    "published_date: 05/25/2022\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Nvidia NGC CLI tool <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "During the prerequisites for this lab, you should have configured your NGC account and retrieved your API key. You also should have logged into the nvcr.io docker account with the `docker login nvcr.io` command. The steps in the cell below further builds on this by downloading the NGC CLI tool for retrieving the docker images used for working with the TAO toolkit. \n",
    "\n",
    "This cell will perform the following actions:\n",
    "* Remove any existing NGC CLI instances\n",
    "* Download the NGC CLI zip file\n",
    "* Unpack the NGC CLI tool and cleanup files\n",
    "* Add the new instance to your PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $PROJECT_DIR/ngccli\n",
    "!unzip -u \"$PROJECT_DIR/ngccli/$CLI\" -d $PROJECT_DIR/ngccli\n",
    "!rm $PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the NGC CLI successfully installed\n",
    "!ngc --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Protocol Buffer Compiler (Protoc) on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Protocol buffer compiler (protoc). This binary will be used in the next cell to compile all the .proto files\n",
    "# under tf-models/research/object_detection/protos/ folder. \n",
    "# https://google.github.io/proto-lens/installing-protoc.html\n",
    "# This installation is for Ubuntu 20.04 OS.\n",
    "\n",
    "%env PROTOC_ZIP=protoc-3.14.0-linux-x86_64.zip\n",
    "!curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.14.0/$PROTOC_ZIP\n",
    "!unzip -o $PROTOC_ZIP -d /usr/local bin/protoc\n",
    "!unzip -o $PROTOC_ZIP -d /usr/local 'include/*'\n",
    "!rm -f $PROTOC_ZIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should install Protobuf compiler on your system which are essential to compile the .proto files. These .proto files would get downloaded in the following cells while we prepare the dataset for training purposes. If the above commands error out due to permission issues, then conncet to your VM via SSH, goto workspace folder and then execute these commands manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download and Prepare Training Data <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "At this point, we have configured our environment for use with the TAO toolkit. Now the fun part begins. We start by downloading the labeled data from Azure storage. For this step you will need a SAS token for downloading the images from blob storage. It is expected that you already have an annotations.json in COCO format which has been exported from the Azure Machine Learning labeling project. We will be using an annotations.json file stored in this repository found [here](workspace/data/annotations.json)\n",
    "\n",
    "### Download Data from Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the python script with the following arguments to download the images\n",
    "!python3 $PROJECT_DIR/models/maskrcnn/scripts/download_aml_blobs.py --output_dir $AML_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, test, and validation sets\n",
    "\n",
    "Next, we run the below Python script to split our images into train, test, and validation sets. We randomly select the images to be put into each of these categories. The main variable to control here is the `--train_pct` argument which determines what percentage of the data is used for training vs testing and validation. \n",
    "\n",
    "![docs/images/train_val_test.jpg](docs/images/train_val_test.jpg)\n",
    "\n",
    "Below is a table summarizing the inputs for the script\n",
    "\n",
    "|Input Parameter|Description|\n",
    "|:--------------|:----------|\n",
    "|--annotations_file|This is an absolute path reference to the `annotations.json` file which was exported from the Azure ML labeling project|\n",
    "|--input_dir|This is an absolute path reference to the directory where the raw image dataset was downloaded to in the previous step|\n",
    "|--output_dir|This is an absolute path reference to the path where the split dataset will reside after it is split|\n",
    "|--train_pct|This determines the percentage of the data to be used for training vs testing and validation|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in preparation for training\n",
    "!python3 $PROJECT_DIR/models/maskrcnn/scripts/split_aml_exported_coco.py \\\n",
    "    --annotations_file $DATA_DIR/annotations.json \\\n",
    "    --input_dir $AML_DATA_DIR \\\n",
    "    --output_dir $DATA_DIR/split-images \\\n",
    "    --train_pct 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert COCO labels to TFRecords\n",
    "\n",
    "The last step in the data preparation process is to convert the images and annotations into TFRecord format. This is done via a helper script which has been modified for this lab. Behind the scenes, this script downloads some additional Tensorflow utilities required for the conversion process and then converts all the data to TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFRecords from coco\n",
    "!bash $PROJECT_DIR/models/maskrcnn/scripts/process-labels-aml.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ignore the following, if the above step was successful. But if the above step errors out due to permission issues while running the protocol buffer compiler (protoc), then connect to your VM via SSH, goto workspace folder and then execute the below command manually:\n",
    "```bash\n",
    "cd tf-models/research && protoc object_detection/protos/*.proto --python_out=.\n",
    "```\n",
    "And then re-run the above step (process-labels-aml.sh) to successfully convert your data into TFRecords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data Directory Structure \n",
    "\n",
    "At this point you have successfully prepared your dataset for use with the TAO toolkit for training an optimized model ready for DeepStream integration. To validate everything went smoothly, your data directory should look like the following:\n",
    "```\n",
    "workspace\n",
    "│   README.md\n",
    "│   file001.txt    \n",
    "│\n",
    "└───data\n",
    "    │   annotations.json\n",
    "    │\n",
    "    └───raw-images\n",
    "    │   │   image1.jpg\n",
    "    │   │   image2.jpg\n",
    "    │   │   ...\n",
    "    │\n",
    "    └───split-images\n",
    "        │   \n",
    "        └───train\n",
    "        │   │   annotations.json\n",
    "        │   └───images\n",
    "        │       │   image1.jpg\n",
    "        │       │   image2.jpg\n",
    "        │       │   ...\n",
    "        │       \n",
    "        └───test\n",
    "        │   │   annotations.json\n",
    "        │   └───images\n",
    "        │       │   image1.jpg\n",
    "        │       │   image2.jpg\n",
    "        │       │   ...│\n",
    "        │\n",
    "        └───val\n",
    "            │   annotations.json\n",
    "            └───images\n",
    "                │   image1.jpg\n",
    "                │   image2.jpg\n",
    "                │   ...        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download pre-trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In an earlier step, we downloaded the Nvidai NGC CLI tool. Now we are ready to use this tool for downloading the pretrained model we will use for transfer learning. The cells below create a new directory for the pretrained model then use the NGC CLI to download the model for use in the TAO toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare directory for model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing instances and create a new empty directory\n",
    "!rm -rf $EXPERIMENT_DIR/pretrained_resnet50/\n",
    "!mkdir -p $EXPERIMENT_DIR/pretrained_resnet50/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available instance segmentation models\n",
    "The below command lists the available instance segmentation models for use with the TAO toolkit. From the output list we are interested in the `resnet50` model which we will download in the next command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the avaialble instance segmentation models\n",
    "!ngc registry model list nvidia/tao/pretrained_instance_segmentation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the resnet50 model to the output directory created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version \\\n",
    "    nvidia/tao/pretrained_instance_segmentation:resnet50 \\\n",
    "    --dest $EXPERIMENT_DIR/pretrained_resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model exists in target directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $EXPERIMENT_DIR/pretrained_resnet50/pretrained_instance_segmentation_vresnet50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Provide the TAO training specification <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "* Tfrecords for the train datasets\n",
    "    * In order to use the newly generated tfrecords, we have updated the dataset_config parameter in the spec file at `$EXPERIMENT_DIR/configs/maskrcnn_train_resnet50.txt`\n",
    "    \n",
    "Note that the learning rate in the spec file is set for 1 GPU training. \n",
    "\n",
    "We have configured this experiment to work with the sample dataset, but you may need to alter these settings if using another dataset. Please refer to the [Nvidia Documentation](https://docs.nvidia.com/tao/tao-toolkit/text/instance_segmentation/mask_rcnn.html) for further information on how to configure the training specification for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the training spec\n",
    "!cat $EXPERIMENT_DIR/configs/maskrcnn_train_resnet50.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train a MaskRCNN model <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "Now we are ready to train our model. As final preparation, we will set a couple environment variables which will be used throughout the tao commands used for the remainder of the lab. A description of these variables is provided below.\n",
    "\n",
    "|ENV Variable|Description|\n",
    "|:-----------|:----------|\n",
    "|KEY|The value provided here is used to encrypt the model|\n",
    "|CONTAINER_EXPERIMENT_DIR|This is the absolute path in the target directory specified in the `~/.tao_mounts.json` file created earlier in this tutorial|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set ENV Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the key variables for the TAO training steps\n",
    "os.environ['KEY'] = \"nvidia_tlt\"\n",
    "os.environ['CONTAINER_EXPERIMENT_DIR'] = \"/workspace/models/maskrcnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove and recreate output directory\n",
    "!sudo rm -rf $EXPERIMENT_DIR/experiment_dir_unpruned\n",
    "!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now we use the command below to train the model using the training spec and output the files to the output directory created above. \n",
    "\n",
    "Notes about training:\n",
    "* The command requires the sample spec file and the output directory location for models\n",
    "* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n",
    "\n",
    "> **WARNING:** The training process will take some time (i.e. several hours to days) to complete depending on how many iterations are in your training spec and the machine you are using for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the MaskRCNN model using resnet50 as starting point\n",
    "!tao mask_rcnn train -e $CONTAINER_EXPERIMENT_DIR/configs/maskrcnn_train_resnet50.txt \\\n",
    "                     -d $CONTAINER_EXPERIMENT_DIR/experiment_dir_unpruned\\\n",
    "                     -k $KEY \\\n",
    "                     --gpus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Training output\n",
    "\n",
    "The command below lists each of the checkpoint files saved during the training process. Each checkpoint is of the pattern \n",
    "\n",
    "`model.step-<checkpoint-number>.tlt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate trained model <a class=\"anchor\" id=\"head-8\"></a>\n",
    "\n",
    "After training the model, we can evaluate the models performance. To do so, we enter the step number we want to run the evaluation for. In this case, we ran our model for 25000 steps so we will use the .tlt file for the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NUM_STEP=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf $EXPERIMENT_DIR/evaluate/\n",
    "!mkdir -p $EXPERIMENT_DIR/evaluate/\n",
    "\n",
    "!tao mask_rcnn evaluate -e $CONTAINER_EXPERIMENT_DIR/configs/maskrcnn_train_resnet50.txt \\\n",
    "                        -m $CONTAINER_EXPERIMENT_DIR/experiment_dir_unpruned/model.step-$NUM_STEP.tlt \\\n",
    "                        -k $KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prune the model <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a model, we will want to prune it to reduce it's size and to improve integration with DeepStream. To run the pruning step we need to specify the below information:\n",
    "\n",
    "#### Pruning inputs\n",
    "- Specify pre-trained model\n",
    "- Output directory to store the pruned model\n",
    "- Threshold for pruning.\n",
    "- A key to save and load the model\n",
    "\n",
    "Usually, you just need to adjust -pth (threshold) for accuracy and model size trade off. Higher pth gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. 0.5 in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory to save the pruned model. \n",
    "# Remove the directory first if it already exists\n",
    "!sudo rm -rf $EXPERIMENT_DIR/experiment_dir_pruned\n",
    "!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform model pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the model and \n",
    "!tao mask_rcnn prune -m $CONTAINER_EXPERIMENT_DIR/experiment_dir_unpruned/model.step-$NUM_STEP.tlt \\\n",
    "                     -o $CONTAINER_EXPERIMENT_DIR/experiment_dir_pruned \\\n",
    "                     -pth 0.5 \\\n",
    "                     -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate pruning output exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that you should retrain the pruned model first, as it cannot be directly used for evaluation or inference. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Retrain pruned models <a class=\"anchor\" id=\"head-10\"></a>\n",
    "\n",
    "After pruning, the model will have lost some accuracy. For that reason, it is necessary to retrain again using the TAO toolkit. The toolkit allows for this by using the pruned model as a starting checkpoint to apply a final trained model for integration with DeepStream. This is achieved through a separate training spec file located in `$EXPERIMENT_DIR/configs/maskrcnn_retrain_resnet50.txt`\n",
    "\n",
    "The inputs for the retraining step are listed below:\n",
    "- Path to the retraining model specification\n",
    "- Output directory to store retrained model\n",
    "- Key used with TAO tookit\n",
    "\n",
    "> **WARNING:** As with the initial training step, this training will take several hours to one day to complete depending on the training spec and the machine you are using for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the training spec\n",
    "!cat $EXPERIMENT_DIR/configs/maskrcnn_retrain_resnet50.txt\n",
    "\n",
    "# prepare the output directories\n",
    "!sudo rm -rf $EXPERIMENT_DIR/experiment_dir_retrain\n",
    "!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain\n",
    "\n",
    "# run the retraining step\n",
    "!tao mask_rcnn train -e $CONTAINER_EXPERIMENT_DIR/configs/maskrcnn_retrain_resnet50.txt \\\n",
    "                     -d $CONTAINER_EXPERIMENT_DIR/experiment_dir_retrain\\\n",
    "                     -k $KEY \\\n",
    "                     --gpus 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate retrained model <a class=\"anchor\" id=\"head-11\"></a>\n",
    "\n",
    "We will once again evaluate the retrained model to make sure the performance is satisfactory. We do so by running the `tao mask_rcnn evaluate` command on the final training checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NUM_STEP=25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao mask_rcnn evaluate -e $CONTAINER_EXPERIMENT_DIR/configs/maskrcnn_retrain_resnet50.txt \\\n",
    "                        -m $CONTAINER_EXPERIMENT_DIR/experiment_dir_retrain/model.step-$NUM_STEP.tlt \\\n",
    "                        -k $KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export the model for use with DeepStream <a class=\"anchor\" id=\"head-12\"></a>\n",
    "\n",
    "This is the final step in the TAO toolkit for preparing your model to be used with DeepStream. The output of the cell below is a .etlt file which can be used with DeepStream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "!tao mask_rcnn export -m $CONTAINER_EXPERIMENT_DIR/experiment_dir_retrain/model.step-$NUM_STEP.tlt \\\n",
    "                      -k $KEY \\\n",
    "                      -e $CONTAINER_EXPERIMENT_DIR/configs/maskrcnn_retrain_resnet50.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate exported model is in output directory\n",
    "\n",
    "We are looking for the `.etlt` file to be present in the output command listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if etlt model is correctly saved.\n",
    "!ls -l $EXPERIMENT_DIR/experiment_dir_retrain/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Run a deepstream pipeline with the model <a class=\"anchor\" id=\"head-13\"></a>\n",
    "\n",
    "The final step in the process is to run a deepstream pipeline using the model. In this instance we are using the `deepstream-app` that is provided as part of the DeepStream sample apps when installing the DeepStream SDK.\n",
    "\n",
    "We have already configured the configuration files required for running this application so it works for this lab exercise, but typically you would need to configure your `pgie_config.txt` and `tracker_config.txt` files according to where your model assets reside and your particular use case.\n",
    "\n",
    "This sample application will use a sample input video file in the `media` folder of this repository and run the video through the DeepStream pipeline. The pipeline will save an output with masks and metadata to the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the deepstream sample app with the config file\n",
    "!deepstream-app -c /home/edwin/repos/manufacturing-demo/deepstream/deepstream_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate output file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the out.mp4 file that was generated by the DeepStream pipeline\n",
    "!ls | grep out.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the output video file\n",
    "\n",
    "<video controls src=\"out.mp4\" width=640 height=480/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
